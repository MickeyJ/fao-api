import pandas as pd
from sqlalchemy.orm import Session
from sqlalchemy.dialects.postgresql import insert as pg_insert
from {{ project_name }}.src.db.utils import load_csv, get_csv_path_for, generate_numeric_id
from {{ project_name }}.src.db.database import run_with_session
from .{{ module.model.table_name }}_model import {{ module.model.model_name }}


# Direct path to synthetic lookup CSV
CSV_PATH = get_csv_path_for("{{ module.file_info.csv_file }}")

table_name = "{{ module.model.table_name }}"


def load():
    """Load the lookup CSV file"""
    return load_csv(CSV_PATH)


def clean(df: pd.DataFrame) -> pd.DataFrame:
    """Clean and prepare lookup data"""
    if df.empty:
        print(f"No {table_name} data to clean.")
        return df

    print(f"\nCleaning {table_name} data...")
    initial_count = len(df)

    # Replace 'nan' strings with None for ALL columns
    df = df.replace({'nan': None, 'NaN': None, 'NAN': None})
    
    # Basic column cleanup
    {% for column in module.model.column_analysis %}
    df['{{ column.csv_column_name }}'] = df['{{ column.csv_column_name }}'].astype(str).str.strip().str.replace("'", "")
    {# Apply format methods for columns #}
    {% if column.format_methods %}
    {% for method in column.format_methods %}
    df['{{ column.csv_column_name }}'] = df['{{ column.csv_column_name }}'].str.{{ method }}()
    {% endfor %}
    {% endif %}
    {# spacer #}
    {% if column.sql_column_name == module.model.pk_sql_column_name %}
    # Keep primary key as string
    {% elif column.inferred_sql_type == 'Integer' %}
    df['{{ column.csv_column_name }}'] = pd.to_numeric(df['{{ column.csv_column_name }}'], errors='coerce')
    {% elif column.inferred_sql_type == 'Float' %}
    df['{{ column.csv_column_name }}'] = pd.to_numeric(df['{{ column.csv_column_name }}'], errors='coerce')
    {% endif %}
    {% endfor %}
    
   
    # Remove any remaining duplicates (shouldn't be any after PK updates)
    df = df.drop_duplicates(subset={{ module.model.hash_columns }}, keep='first')
    
    # Drop any rows with null PKs
    df = df.dropna(subset=['{{ module.model.pk_column }}'])
    
    final_count = len(df)
    print(f"  Cleaned: {initial_count} → {final_count} rows")
    return df


def insert(df: pd.DataFrame, session: Session):
    """Insert lookup data - simple bulk insert since conflicts are already resolved"""
    if df.empty:
        print(f"No {table_name} data to insert.")
        return
    
    print(f"\nInserting {table_name} data...")
    
    records = []
    for _, row in df.iterrows():
        record = {}
        
        # Generate hash ID from configured columns
        hash_columns = {{ module.model.hash_columns | tojson }}
        record['id'] = generate_numeric_id(row.to_dict(), hash_columns)
        {% for column in module.model.column_analysis %}
        record['{{ column.sql_column_name }}'] = row['{{ column.csv_column_name }}']
        {% endfor %}
        records.append(record)
    
    if records:
        try:
            stmt = pg_insert({{ module.model.model_name }}).values(records)
            stmt = stmt.on_conflict_do_nothing()
            result = session.execute(stmt)
            session.commit()
            print(f"  ✅ Inserted {result.rowcount} rows")
        except Exception as e:
            print(f"  ❌ Error during bulk insert: {e}")
            session.rollback()
            raise
    
    print(f"✅ {table_name} insert complete")


def run(db):
    """Run the complete ETL pipeline for this lookup table"""
    df = load()
    df = clean(df)
    insert(df, db)


if __name__ == "__main__":
    run_with_session(run)